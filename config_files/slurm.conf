
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=10.129.131.211
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/pgid
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurm/
SwitchType=switch/none
TaskPlugin=task/cgroup
#
#
#Prolog=/home/ub-10/sandeep/prolog.sh
AuthType=auth/munge
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=10.129.131.211
#AccountingStorageTRES=cpu,mem,energy,node,billing,fs/disk,vmem,pages,gres/gpu
ClusterName=testcluster
#JobAcctGatherFrequency=0.2
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherParams=UsePss,EnableTRESbilling
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
#
#
# COMPUTE NODES
#NodeName=gtx9701 NodeAddr=10.129.131.147 Gres=gpu:1,mps:100 CPUs=4 State=idle
NodeName=ub-10 CPUs=64 Gres=gpu:1,mps:150 State=idle
#NodeName=synerg CPUs=64 Gres=gpu:1,mps:100 State=idle
# NodeName=linux[1-32] CPUs=1 State=UNKNOWN
# NodeName=linux1 NodeAddr=128.197.115.158 CPUs=4 State=UNKNOWN
# NodeName=linux2 NodeAddr=128.197.115.7 CPUs=4 State=UNKNOWN
GresTypes=mps,gpu
#PartitionName=test Nodes=ub-10 Default=YES MaxTime=INFINITE State=UP
PartitionName=test Nodes=ALL
#PartitionName=test Nodes=synerg,gtx9701 Default=YES MaxTime=INFINITE State=UP
DebugFlags=NO_CONF_HASH
# DefMemPerNode=1000
# MaxMemPerNode=1000
# DefMemPerCPU=4000 
# MaxMemPerCPU=4096
#ReconfigFlags=PARTITIONS|NODES
